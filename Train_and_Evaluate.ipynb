{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a539696",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting torchvisionNote: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "  Downloading torchvision-0.16.2-cp311-cp311-win_amd64.whl (1.1 MB)\n",
      "     ---------------------------------------- 1.1/1.1 MB 1.6 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (1.26.2)\n",
      "Requirement already satisfied: requests in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (2.31.0)\n",
      "Requirement already satisfied: torch==2.1.2 in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (2.1.2)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torchvision) (10.1.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.1.2->torchvision) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.1.2->torchvision) (4.9.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.1.2->torchvision) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.1.2->torchvision) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.1.2->torchvision) (3.1.2)\n",
      "Requirement already satisfied: fsspec in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch==2.1.2->torchvision) (2023.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchvision) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchvision) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchvision) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->torchvision) (2023.11.17)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch==2.1.2->torchvision) (2.1.3)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch==2.1.2->torchvision) (1.3.0)\n",
      "Installing collected packages: torchvision\n",
      "Successfully installed torchvision-0.16.2\n"
     ]
    }
   ],
   "source": [
    "%pip install torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "636ec4fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pytorch_lightningNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached pytorch_lightning-2.1.3-py3-none-any.whl (777 kB)\n",
      "Requirement already satisfied: numpy>=1.17.2 in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pytorch_lightning) (1.26.2)\n",
      "Requirement already satisfied: torch>=1.12.0 in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pytorch_lightning) (2.1.2)\n",
      "Requirement already satisfied: tqdm>=4.57.0 in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pytorch_lightning) (4.66.1)\n",
      "Requirement already satisfied: PyYAML>=5.4 in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pytorch_lightning) (6.0.1)\n",
      "Requirement already satisfied: fsspec[http]>=2022.5.0 in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pytorch_lightning) (2023.12.2)\n",
      "Collecting torchmetrics>=0.7.0\n",
      "  Downloading torchmetrics-1.3.0-py3-none-any.whl (840 kB)\n",
      "     ------------------------------------ 840.2/840.2 kB 505.8 kB/s eta 0:00:00\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\riddhi\\appdata\\roaming\\python\\python311\\site-packages (from pytorch_lightning) (23.2)\n",
      "Requirement already satisfied: typing-extensions>=4.0.0 in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from pytorch_lightning) (4.9.0)\n",
      "Collecting lightning-utilities>=0.8.0\n",
      "  Using cached lightning_utilities-0.10.0-py3-none-any.whl (24 kB)\n",
      "Requirement already satisfied: requests in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from fsspec[http]>=2022.5.0->pytorch_lightning) (2.31.0)\n",
      "Collecting aiohttp!=4.0.0a0,!=4.0.0a1\n",
      "  Downloading aiohttp-3.9.1-cp311-cp311-win_amd64.whl (364 kB)\n",
      "     ------------------------------------ 364.8/364.8 kB 197.4 kB/s eta 0:00:00\n",
      "Requirement already satisfied: setuptools in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from lightning-utilities>=0.8.0->pytorch_lightning) (65.5.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.12.0->pytorch_lightning) (3.13.1)\n",
      "Requirement already satisfied: sympy in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.12.0->pytorch_lightning) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.12.0->pytorch_lightning) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from torch>=1.12.0->pytorch_lightning) (3.1.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\riddhi\\appdata\\roaming\\python\\python311\\site-packages (from tqdm>=4.57.0->pytorch_lightning) (0.4.6)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]>=2022.5.0->pytorch_lightning) (23.2.0)\n",
      "Collecting multidict<7.0,>=4.5\n",
      "  Downloading multidict-6.0.4-cp311-cp311-win_amd64.whl (28 kB)\n",
      "Collecting yarl<2.0,>=1.0\n",
      "  Downloading yarl-1.9.4-cp311-cp311-win_amd64.whl (76 kB)\n",
      "     -------------------------------------- 76.7/76.7 kB 609.9 kB/s eta 0:00:00\n",
      "Collecting frozenlist>=1.1.1\n",
      "  Downloading frozenlist-1.4.1-cp311-cp311-win_amd64.whl (50 kB)\n",
      "     -------------------------------------- 50.5/50.5 kB 116.9 kB/s eta 0:00:00\n",
      "Collecting aiosignal>=1.1.2\n",
      "  Downloading aiosignal-1.3.1-py3-none-any.whl (7.6 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from jinja2->torch>=1.12.0->pytorch_lightning) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2.1.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from requests->fsspec[http]>=2022.5.0->pytorch_lightning) (2023.11.17)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\riddhi\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from sympy->torch>=1.12.0->pytorch_lightning) (1.3.0)\n",
      "Installing collected packages: multidict, lightning-utilities, frozenlist, yarl, aiosignal, torchmetrics, aiohttp, pytorch_lightning\n",
      "Successfully installed aiohttp-3.9.1 aiosignal-1.3.1 frozenlist-1.4.1 lightning-utilities-0.10.0 multidict-6.0.4 pytorch_lightning-2.1.3 torchmetrics-1.3.0 yarl-1.9.4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip available: 22.3.1 -> 23.3.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "%pip install pytorch_lightning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cosmetic-healthcare",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchvision\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imgaug.augmenters as iaa\n",
    "from dataset import CardiacDataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "stock-switch",
   "metadata": {},
   "source": [
    "Creating the dataset objects and the augmentation parameters to specify the augmentation parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "advanced-filing",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_root_path = \"E:\\\\Cardiac Detection\\\\ProcessedHeart\\\\train\\\\\"\n",
    "train_subjects = \"E:\\\\Cardiac Detection\\\\npnp\\\\train.npy\"\n",
    "val_root_path = \"E:\\\\Cardiac Detection\\\\ProcessedHeart\\\\val\\\\\"\n",
    "val_subjects = \"E:\\\\Cardiac Detection\\\\npnp\\\\val.npy\"\n",
    "\n",
    "train_transforms = iaa.Sequential([\n",
    "                                iaa.GammaContrast(),\n",
    "                                iaa.Affine(\n",
    "                                    scale=(0.8, 1.2),\n",
    "                                    rotate=(-10, 10),\n",
    "                                    translate_px=(-10, 10)\n",
    "                                )\n",
    "                            ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "considered-august",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CardiacDataset(\"E:\\\\Cardiac Detection\\\\05-Detection\\\\rsna_heart_detection.csv\", train_subjects, train_root_path, train_transforms)\n",
    "val_dataset = CardiacDataset(\"E:\\\\Cardiac Detection\\\\05-Detection\\\\rsna_heart_detection.csv\", val_subjects, val_root_path, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "insured-optimum",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "num_workers = 4\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "solved-uruguay",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 400 train images and 96 val images\n"
     ]
    }
   ],
   "source": [
    "train_dataset = CardiacDataset(\n",
    "    \"E:\\\\Cardiac Detection\\\\05-Detection\\\\rsna_heart_detection.csv\",\n",
    "     train_subjects,\n",
    "     train_root_path,\n",
    "     augs = train_transforms)\n",
    "\n",
    "val_dataset = CardiacDataset(\n",
    "    \"E:\\\\Cardiac Detection\\\\05-Detection\\\\rsna_heart_detection.csv\",\n",
    "     val_subjects,\n",
    "     val_root_path,\n",
    "     augs=None)\n",
    "\n",
    "print(f\"There are {len(train_dataset)} train images and {len(val_dataset)} val images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "massive-beast",
   "metadata": {},
   "source": [
    "## Adapting batch size and num_workers according to your computing hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "indirect-loading",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8#TODO\n",
    "num_workers = 4# TODO\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                           num_workers=num_workers, shuffle=True)\n",
    "val_loader = torch.utils.data.DataLoader(val_dataset, batch_size=batch_size, num_workers=num_workers, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "known-forum",
   "metadata": {},
   "source": [
    "## Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "hollow-friendship",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardiacDetectionModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = torchvision.models.resnet18(pretrained=True)\n",
    "        self.model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.model.fc = torch.nn.Linear(in_features=512 ,out_features=4)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "        \n",
    "    def forward(self, data):\n",
    "        return self.model(data)\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_ray, label = batch\n",
    "        label = label.float()\n",
    "        pred = self(x_ray)\n",
    "        loss = self.loss_fn(pred, label)\n",
    "        \n",
    "        self.log(\"Train Loss\", loss)\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            self.log_images(x_ray.cpu(), pred.cpu(), label.cpu(), \"Train\")\n",
    "        return loss\n",
    "    \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        x_ray, label = batch\n",
    "        label = label.float()\n",
    "        pred = self(x_ray)\n",
    "        loss = self.loss_fn(pred, label)\n",
    "        \n",
    "        self.log(\"Val Loss\", loss)\n",
    "        \n",
    "        if batch_idx % 50 == 0:\n",
    "            self.log_images(x_ray.cpu(), pred.cpu(), label.cpu(), \"Val\")\n",
    "        return loss\n",
    "    \n",
    "    def log_images(self, x_ray, pred, label, name):\n",
    "        results = []\n",
    "        \n",
    "        for i in range(4):\n",
    "            coords_labels = label[i]\n",
    "            coords_pred = pred[i]\n",
    "            \n",
    "            img = ((x_ray[i] * 0.252)+0.494).numpy()[0]\n",
    "            \n",
    "            x0, y0 = coords_labels[0].int().item(), coords_labels[1].int().item()\n",
    "            x1, y1 = coords_labels[2].int().item(), coords_labels[3].int().item()\n",
    "            img = cv2.rectangle(img, (x0, y0), (x1, y1), (0, 0, 0), 2)\n",
    "            \n",
    "            x0, y0 = coords_pred[0].int().item(), coords_pred[1].int().item()\n",
    "            x1, y1 = coords_pred[2].int().item(), coords_pred[3].int().item()\n",
    "            img = cv2.rectangle(img, (x0, y0), (x1, y1), (1, 1, 1), 2)\n",
    "            \n",
    "            results.append(torch.tensor(img).unsqueeze(0))\n",
    "        \n",
    "        grid = torchvision.utils.make_grid(results, 2)\n",
    "        self.logger.experiment.add_image(name, grid, self.global_step)\n",
    "        \n",
    "    def configure_optimizers(self):\n",
    "        #Caution! You always need to return a list here (just pack your optimizer into one :))\n",
    "        return [self.optimizer]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "neutral-square",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CardiacDetectionModel(pl.LightningModule):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.model = torchvision.models.resnet18(pretrained=True)\n",
    "        \n",
    "        # Change conv1 from 3 to 1 input channels\n",
    "        self.model.conv1 = torch.nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        \n",
    "        # Change out_feature of the last fully connected layer (called fc in resnet18) from 1000 to 4\n",
    "        self.model.fc = torch.nn.Linear(in_features=512, out_features=4)\n",
    "        \n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=1e-4)\n",
    "        self.loss_fn = torch.nn.MSELoss()\n",
    "    \n",
    "    def forward(self, data):\n",
    "        pred = self.model(data)\n",
    "        return pred\n",
    "    \n",
    "    def training_step(self, batch, batch_idx):\n",
    "        x_ray, label = batch\n",
    "        label = label.float()  # Convert label to float (just needed for loss computation)\n",
    "        pred = self(x_ray)\n",
    "        loss = self.loss_fn(pred, label)  # Compute the loss\n",
    "        \n",
    "        # Log loss\n",
    "        self.log(\"Train Loss\", loss)\n",
    "        if batch_idx % 50 == 0:\n",
    "            self.log_images(x_ray.cpu(), pred.cpu(), label.cpu(), \"Train\")\n",
    "\n",
    "        return loss\n",
    "    \n",
    "        \n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        # Same steps as in the training_step\n",
    "        x_ray, label = batch\n",
    "        label = label\n",
    "\n",
    "        label = label.float()  # Convert label to float (just needed for loss computation)\n",
    "        pred = self(x_ray)\n",
    "        \n",
    "        loss = self.loss_fn(pred, label)\n",
    "        self.log(\"Val Loss\", loss)\n",
    "        if batch_idx % 50 == 0:\n",
    "            self.log_images(x_ray.cpu(), pred.cpu(), label.cpu(), \"Val\")\n",
    "        return loss\n",
    "    \n",
    "    def log_images(self, x_ray, pred, label, name):\n",
    "        results = []\n",
    "        \n",
    "        # Here we create a grid consisting of 4 predictions\n",
    "        for i in range(4):\n",
    "            coords_labels = label[i]\n",
    "            coords_pred = pred[i]\n",
    "            img = ((x_ray[i] * 0.252)+0.494).numpy()[0]\n",
    "            \n",
    "            # Extract the coordinates from the label\n",
    "            x0, y0 = coords_labels[0].int().item(), coords_labels[1].int().item()\n",
    "            x1, y1 = coords_labels[2].int().item(), coords_labels[3].int().item()\n",
    "            img = cv2.rectangle(img, (x0, y0), (x1, y1), (0, 0, 0), 2)\n",
    "            \n",
    "            # Extract the coordinates from the prediction           \n",
    "            x0, y0 = coords_pred[0].int().item(), coords_pred[1].int().item()\n",
    "            x1, y1 = coords_pred[2].int().item(), coords_pred[3].int().item()\n",
    "            img = cv2.rectangle(img, (x0, y0), (x1, y1), (1, 1, 1), 2)\n",
    "            \n",
    "            \n",
    "            results.append(torch.tensor(img).unsqueeze(0))\n",
    "        grid = torchvision.utils.make_grid(results, 2)\n",
    "        self.logger.experiment.add_image(f\"{name} Prediction vs Label\", grid, self.global_step)\n",
    "\n",
    "            \n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        #Caution! You always need to return a list here (just pack your optimizer into one :))\n",
    "        return [self.optimizer]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "second-stuart",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riddhi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\riddhi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n",
      "Downloading: \"https://download.pytorch.org/models/resnet18-f37072fd.pth\" to C:\\Users\\riddhi/.cache\\torch\\hub\\checkpoints\\resnet18-f37072fd.pth\n",
      "100%|██████████| 44.7M/44.7M [00:33<00:00, 1.39MB/s]\n"
     ]
    }
   ],
   "source": [
    "# Create the model object\n",
    "model = CardiacDetectionModel()  # Instanciate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "becoming-catch",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the checkpoint callback\n",
    "checkpoint_callback = ModelCheckpoint(\n",
    "    monitor='Val Loss',\n",
    "    save_top_k=10,\n",
    "    mode='min')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "realistic-cylinder",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "extended-playing",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    }
   ],
   "source": [
    "# Create the trainer\n",
    "# Change the gpus parameter to the number of available gpus in your computer. Use 0 for CPU training\n",
    "\n",
    "gpus = 1 #TODO\n",
    "trainer = pl.Trainer(accelerator=\"auto\", logger=TensorBoardLogger(\"E:\\\\Cardiac Detection\\\\05-Detection\\\\loglog\"), log_every_n_steps=1,\n",
    "                     default_root_dir=\"E:\\\\Cardiac Detection\\\\05-Detection\\weightsss\", callbacks=checkpoint_callback,\n",
    "                     max_epochs=100)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "massive-bunch",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Missing logger folder: ./logs\\lightning_logs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\riddhi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  | Name    | Type    | Params\n",
      "------------------------------------\n",
      "0 | model   | ResNet  | 11.2 M\n",
      "1 | loss_fn | MSELoss | 0     \n",
      "------------------------------------\n",
      "11.2 M    Trainable params\n",
      "0         Non-trainable params\n",
      "11.2 M    Total params\n",
      "44.689    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0a629790749d47aeaa2effebd11b1420",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riddhi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'val_dataloader' to speed up the dataloader worker initialization.\n",
      "c:\\Users\\riddhi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:436: Consider setting `persistent_workers=True` in 'train_dataloader' to speed up the dataloader worker initialization.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "88ba989eb47b4f0297de6d79a6a54c6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |          | 0/? [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\riddhi\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "# Train the detection model\n",
    "trainer.fit(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "whole-designer",
   "metadata": {},
   "source": [
    "## Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "listed-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atmospheric-royalty",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "model = model.load_from_checkpoint(\"weight.ckpt\")\n",
    "model.eval();\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "complete-contemporary",
   "metadata": {},
   "source": [
    "## Computing prediction for all validation samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "labeled-witness",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "preds = []\n",
    "labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for data, label in val_dataset:\n",
    "        data = data.to(device).float().unsqueeze(0)\n",
    "        pred = model(data)[0].cpu()\n",
    "        preds.append(pred)\n",
    "        labels.append(label)\n",
    "        \n",
    "preds=torch.stack(preds)\n",
    "labels=torch.stack(labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "residential-cleanup",
   "metadata": {},
   "source": [
    "## Computing mean deviation between prediction and labels for each coordinate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "stainless-construction",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "abs(preds-labels).mean(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mexican-video",
   "metadata": {},
   "source": [
    "## Example prediction:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "interim-shield",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "IDX = 9\n",
    "img, label = val_dataset[IDX]\n",
    "current_pred = preds[IDX]\n",
    "\n",
    "fig, axis = plt.subplots(1, 1)\n",
    "axis.imshow(img[0], cmap=\"bone\")\n",
    "heart = patches.Rectangle((current_pred[0], current_pred[1]), current_pred[2]-current_pred[0],\n",
    "                          current_pred[3]-current_pred[1], linewidth=1, edgecolor='r', facecolor='none')\n",
    "axis.add_patch(heart)\n",
    "\n",
    "print(label)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
